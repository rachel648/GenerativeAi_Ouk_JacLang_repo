# Transformer Training Configuration
batch_size: 16
learning_rate: 0.0001
num_epochs: 50
device: "cuda"
seed: 42
save_interval: 5
log_interval: 100
checkpoint_dir: "checkpoints/transformer"
log_dir: "logs/transformer"
data_dir: "data/text"

# Transformer-specific parameters
vocab_size: 10000
seq_length: 512
d_model: 512
num_heads: 8
num_layers: 6
d_ff: 2048
dropout: 0.1

# Training parameters
warmup_steps: 4000
max_lr: 0.001
label_smoothing: 0.1